# -*- coding: utf-8 -*-
"""Exercise_3_Time_Series.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-KxT6fKXAm_GhbgfV-NwJDInaumfZWoz
"""
"first install this be4 running the this task"
"!pip install Aeon"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import torch
from torch import nn
import torch.nn.functional as F
import torch.utils.data as data_utils
import torch.optim as optim
from aeon.datasets import load_classification
import warnings
warnings.filterwarnings('ignore')

print("Loading ECG5000 dataset from Aeon...")
try:
    X_train, y_train = load_classification("ECG5000", split="train")
    X_test, y_test = load_classification("ECG5000", split="test")
except:
    print("Downloading dataset... (first time only)")
    X_train, y_train = load_classification("ECG5000", split="train")
    X_test, y_test = load_classification("ECG5000", split="test")

print(f"Dataset loaded: {X_train.shape}, Classes: {np.unique(y_train)}")

# Convert to numpy arrays
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).astype(int)
y_test = np.array(y_test).astype(int)

# Map classes to 0, 1, 2...
unique_classes = np.unique(y_train)
class_mapping = {old: new for new, old in enumerate(unique_classes)}
y_train = np.array([class_mapping[y] for y in y_train])
y_test = np.array([class_mapping[y] for y in y_test])

print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")
print(f"Number of classes: {len(np.unique(y_train))}")

class TimeSeriesClassifier(nn.Module):
    def __init__(self, inp_units, num_units=256, num_units1=128,
                 out_units=None, nonlin=F.relu):
        super(TimeSeriesClassifier, self).__init__()
        if out_units is None:
            raise ValueError("Must provide out_units (number of classes)")
        self.dense0 = nn.Linear(inp_units, num_units)
        self.dense1 = nn.Linear(num_units, num_units)
        self.dense2 = nn.Linear(num_units, num_units1)
        self.dropout = nn.Dropout(0.3)
        self.output = nn.Linear(num_units1, out_units)
        self.nonlin = nonlin

    def forward(self, X):
        X = self.nonlin(self.dense0(X))
        X = self.dropout(X)
        X = self.nonlin(self.dense1(X))
        X = self.dropout(X)
        X = self.nonlin(self.dense2(X))
        X = self.output(X)
        return X

def train_model(X_train, y_train, X_test, y_test, normalize=False,
                epochs=150, batch_size=16):
    """Train model with or without normalization"""

    # Flatten time series FIRST (before any scaling)
    if X_train.ndim == 3:
        X_train = X_train.reshape(X_train.shape[0], -1)
        X_test = X_test.reshape(X_test.shape[0], -1)
        print("Flattened input:", X_train.shape)

    # Apply standardization if requested
    if normalize:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        print("✓ Standardization applied")
    else:
        X_train_scaled = X_train
        X_test_scaled = X_test
        print("✗ No standardization")

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train_scaled).float()
    y_train_tensor = torch.tensor(y_train).long()

    print(f"X_train_tensor shape: {X_train_tensor.shape}")
    print(f"y_train_tensor shape: {y_train_tensor.shape}")

    # Create data loader
    train_data = data_utils.TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = data_utils.DataLoader(
        dataset=train_data,
        batch_size=batch_size,
        shuffle=True
    )

    # Initialize model
    num_classes = len(class_mapping)
    input_size = X_train.shape[1]

    net = TimeSeriesClassifier(
        inp_units=input_size,
        num_units=256,
        num_units1=128,
        out_units=num_classes
    )

    net.to(device)

    # Optimizer and loss
    optimizer = optim.Adam(net.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.9,
        patience=15,
    )


    # Training loop
    train_losses = []
    for epoch in range(epochs):
        train_loss = 0.0

        if epoch > 0:
            scheduler.step(train_loss)

        for xd, yd in train_loader:
            xd = xd.to(device)
            yd = yd.to(device)

            output = net(xd)
            optimizer.zero_grad()
            loss = criterion(output, yd)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        avg_loss = train_loss / len(train_loader)
        train_losses.append(avg_loss)

        if (epoch + 1) % 30 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}, "
                  f"LR: {optimizer.param_groups[0]['lr']:.6f}")

    # Testing
    X_test_tensor = torch.tensor(X_test_scaled).float().to(device)
    with torch.no_grad():
        predictions_raw = net(X_test_tensor)

    predictions_raw = predictions_raw.cpu().numpy()
    y_pred = np.argmax(predictions_raw, axis=1)

    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    return {
        'accuracy': accuracy,
        'confusion_matrix': cm,
        'predictions': y_pred,
        'train_losses': train_losses,
        'normalize': normalize
    }

print("\n" + "="*70)
print("TRAINING WITHOUT NORMALIZATION")
print("="*70)
results_no_norm = train_model(X_train, y_train, X_test, y_test,normalize=False, epochs=150)

print("\n" + "="*70)
print("TRAINING WITH STANDARDIZATION")
print("="*70)
results_norm = train_model(X_train, y_train, X_test, y_test,
                           normalize=True, epochs=150)

print("\n" + "="*70)
print("RESULTS COMPARISON")
print("="*70)

accuracy_no_norm = results_no_norm['accuracy']
accuracy_norm = results_norm['accuracy']
improvement = (accuracy_norm - accuracy_no_norm) * 100

print(f"\nAccuracy WITHOUT normalization: {accuracy_no_norm*100:.2f}%")
print(f"Accuracy WITH normalization:    {accuracy_norm*100:.2f}%")
print(f"Improvement: {improvement:.2f} percentage points")

cm_no_norm = results_no_norm['confusion_matrix']
cm_norm = results_norm['confusion_matrix']

fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Confusion Matrix - No Normalization
sns.heatmap(cm_no_norm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],
            cbar_kws={'label': 'Count'})
axes[0, 0].set_title(f'Confusion Matrix - No Normalization\n'
                     f'Accuracy: {accuracy_no_norm*100:.2f}%', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Predicted')
axes[0, 0].set_ylabel('Actual')

# Confusion Matrix - With Normalization
sns.heatmap(cm_norm, annot=True, fmt='d', cmap='Greens', ax=axes[0, 1],
            cbar_kws={'label': 'Count'})
axes[0, 1].set_title(f'Confusion Matrix - With Normalization\n'
                     f'Accuracy: {accuracy_norm*100:.2f}%', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Predicted')
axes[0, 1].set_ylabel('Actual')

# Training Loss Comparison
axes[1, 0].plot(results_no_norm['train_losses'], label='No Normalization', linewidth=2)
axes[1, 0].plot(results_norm['train_losses'], label='With Normalization', linewidth=2)
axes[1, 0].set_xlabel('Epoch', fontsize=11)
axes[1, 0].set_ylabel('Training Loss', fontsize=11)
axes[1, 0].set_title('Training Loss Comparison', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()